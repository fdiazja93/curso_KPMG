{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6f0019",
   "metadata": {},
   "source": [
    "# Clasificación con regresión logística y árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14dd2f",
   "metadata": {},
   "source": [
    "Este notebook está dividido en tres partes, y tiene como objetivo explorar cómo construir y evaluar modelos de clasificación utilizando tres algoritmos fundamentales: regresión logística, árboles de decisión y random forest. Comenzaremos con una implementación desde cero del algoritmo de regresión logística, aplicado a la detección de cáncer de mama, y luego compararemos su desempeño con la versión integrada en scikit-learn. Posteriormente, utilizaremos árboles de decisión para modelar el mismo problema, y analizaremos cómo los hiperparámetros afectan su capacidad de generalización. Finalmente, emplearemos el algoritmo de random forest, una técnica de ensamblaje que combina múltiples árboles de decisión para mejorar la robustez y el rendimiento predictivo del modelo. A lo largo del notebook, haremos énfasis en buenas prácticas de validación cruzada, evaluación con métricas apropiadas (como el F1 score), y detección de posibles problemas de sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f7d7c",
   "metadata": {},
   "source": [
    "## Parte 1: Regresión Logística\n",
    "\n",
    "La **regresión logística** es un modelo de clasificación supervisada que se utiliza para predecir probabilidades asociadas a clases binarias (por ejemplo, 0 o 1, Positivo o Negativo, Sí o No). A diferencia de la regresión lineal, la salida del modelo no es un número real continuo, sino una probabilidad entre 0 y 1.\n",
    "\n",
    "Para este modelo escogemos la siguiente hipótesis que predice probabilidades de que puntos de datos pertenezcan a una clase:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z) = \\sigma(\\mathbf{x}^\\top \\mathbf{w} + b)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $\\hat{y}$ es la probabilidad predicha de pertenecer a la clase 1,\n",
    "- $\\mathbf{x}$ es el vector de características o features de entrada,\n",
    "- $\\mathbf{w}$ son los pesos del modelo,\n",
    "- $b$ es el sesgo o bias,\n",
    "- $\\sigma(\\cdot)$ es la **función sigmoide**, definida como:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Esta función transforma cualquier número real en el intervalo $(0, 1)$, permitiendo interpretarlo como una probabilidad.\n",
    "\n",
    "La regresión logística se entrena minimizando la **entropía cruzada binaria** (también conocida como log-loss):\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $m$ es el número de ejemplos de entrenamiento,\n",
    "- $y^{(i)}$ es la clase verdadera (0 o 1),\n",
    "- $\\hat{y}^{(i)}$ es la probabilidad predicha para la clase 1.\n",
    "\n",
    "\n",
    "\n",
    "Para medir el desempeño de los modelos de clasifiación binaria, comunmente se usa la **matriz de confusión**: una tabla que resume el rendimiento del modelo. Se organiza de la siguiente forma:\n",
    "\n",
    "|                       | Predicción: 0 | Predicción: 1 |\n",
    "|-----------------------|---------------|---------------|\n",
    "| **Real: 0**           | TN (verdaderos negativos) | FP (falsos positivos) |\n",
    "| **Real: 1**           | FN (falsos negativos)     | TP (verdaderos positivos) |\n",
    "\n",
    "- **TP (True Positives):** el modelo predijo 1 y era 1.  \n",
    "- **TN (True Negatives):** el modelo predijo 0 y era 0.  \n",
    "- **FP (False Positives):** el modelo predijo 1 pero era 0.  \n",
    "- **FN (False Negatives):** el modelo predijo 0 pero era 1.\n",
    "\n",
    "Para tener una medida única de rendimiento, se usa el **F1 score**. Esta es una métrica de rendimiento que combina la **precisión** (“¿De todas las veces que el modelo predijo la clase positiva (1), cuántas veces acertó?”) y la **recuperación** (“¿De todos los casos realmente positivos (1), cuántos detectó el modelo?”) en un solo número. Es especialmente útil cuando los datos están desequilibrados.\n",
    "\n",
    "Se define como:\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{precisión} \\cdot \\text{recuperación}}{\\text{precisión} + \\text{recuperación}}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $\\text{precisión} = \\frac{TP}{TP + FP}$  \n",
    "  (de todas las veces que el modelo predijo 1, ¿cuántas fueron correctas?),\n",
    "  \n",
    "- $\\text{recuperación} = \\frac{TP}{TP + FN}$  \n",
    "  (de todos los verdaderos casos positivos, ¿cuántos encontró el modelo?).\n",
    "\n",
    "Un valor de F1 cercano a 1 indica que el modelo tiene un buen equilibrio entre precisión y recuperación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45992c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías necesarias\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede760eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el dataset de cancer de mama y lo convertimos a DataFrame de pandas\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "df_cancer = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "df_cancer['target'] = cancer.target\n",
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491b788",
   "metadata": {},
   "source": [
    "**Importante: Generalmente es necesario hacer un análisis detallado de todos los datos, ver sus estadísticas, graficarlos, chequear que los datos estén balanceados. También es buena práctica entender a fondo el dataset y sus características para identificar características redundantes. Esto no lo haremos hoy aquí** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83231665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO: Usa los métodos de pandas para ver explorar el DataFrame\n",
    "#df_cancer.describe()\n",
    "#df_cancer.info()\n",
    "#df_cancer.shape\n",
    "#df_cancer.dtypes.unique()\n",
    "#df_cancer.isnull()\n",
    "#df_cancer.isnull().sum()\n",
    "#df_cancer['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25b26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \"\"\"\n",
    "    Implementación personalizada de regresión logística usando descenso de gradiente.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa los parámetros del modelo.\n",
    "        \"\"\"\n",
    "        self.w = None \n",
    "        self.b = None \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        Calcula la función sigmoide.\n",
    "        \n",
    "        Args:\n",
    "            z (numpy.ndarray): Arreglo de entrada.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Resultado de aplicar la sigmoide elemento por elemento.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z)) \n",
    "    \n",
    "    def entrenar(self, x_train, y_train, alpha, iter=1000):\n",
    "        \"\"\"\n",
    "        Entrena el modelo de regresión logística usando descenso de gradiente.\n",
    "        \n",
    "        Args:\n",
    "            x_train (numpy.ndarray): Características de entrenamiento.\n",
    "            y_train (numpy.ndarray): Etiquetas de entrenamiento.\n",
    "            alpha (float): Tasa de aprendizaje.\n",
    "            iter (int): Número de iteraciones.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (w, b) Vector de pesos y sesgo optimizados.\n",
    "        \"\"\"\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train).flatten()\n",
    "\n",
    "        if x_train.ndim == 1:\n",
    "            x_train = x_train.reshape(-1, 1) \n",
    "\n",
    "        m, n = x_train.shape \n",
    "        w = np.zeros(n)  \n",
    "        b = 0. \n",
    "\n",
    "        for _ in range(iter):\n",
    "            z = x_train @ w + b \n",
    "            h = LogReg.sigmoid(z)  \n",
    "            dcost_dw = (1 / m) * (x_train.T @ (h - y_train))\n",
    "            dcost_db = (1 / m) * np.sum(h - y_train)\n",
    "            w -= alpha * dcost_dw\n",
    "            b -= alpha * dcost_db\n",
    "\n",
    "        self.w = w \n",
    "        self.b = b\n",
    "        return self.w, self.b\n",
    "    \n",
    "    def predecir(self, x):\n",
    "        \"\"\"\n",
    "        Predice etiquetas de clase usando el modelo entrenado.\n",
    "        \n",
    "        Args:\n",
    "            x (numpy.ndarray): Datos de entrada.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Etiquetas predichas (0 o 1).\n",
    "        \"\"\"\n",
    "        probabilidades = LogReg.sigmoid(x @ self.w + self.b)  \n",
    "        predicciones = probabilidades > 0.5  \n",
    "        return predicciones.astype(int) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15483a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos los datos para el modelo\n",
    "\n",
    "X_cancer = df_cancer.drop(columns=['target']).to_numpy()\n",
    "y_cancer = df_cancer['target'].to_numpy()\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "X_cancer_train, X_cancer_test, y_cancer_train, y_cancer_test = train_test_split(X_cancer, y_cancer, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalamos los datos\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X_cancer_train)\n",
    "X_train_scaled = scaler.transform(X_cancer_train)\n",
    "X_test_scaled = scaler.transform(X_cancer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b2e87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.22049866,  0.01786638, -0.23164502, -0.23792155,  0.13485344,\n",
       "        -0.18186592, -0.33299901, -0.38794065,  0.07775716,  0.23141199,\n",
       "        -0.13102749,  0.1519092 , -0.12003654, -0.1252357 ,  0.17053353,\n",
       "         0.01011835,  0.00337892,  0.01117037,  0.19063616,  0.07079685,\n",
       "        -0.30010121, -0.01648601, -0.29387164, -0.26403418,  0.06103145,\n",
       "        -0.19823976, -0.25359031, -0.38245854, -0.00939993,  0.00667793]),\n",
       " np.float64(0.8630419169674644))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una instancia del modelo de regresión logística y lo entrenamos con los datos escalados\n",
    "\n",
    "mi_modelo = LogReg()\n",
    "mi_modelo.entrenar(X_train_scaled, y_cancer_train, alpha=0.01, iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314c3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tenemos un accuracy de: 0.93 ===\n",
      "\n",
      "== Tenemos 35 verdaderos negativos ==\n",
      "\n",
      "== Tenemos 71 verdaderos positivos ==\n",
      "\n",
      "== Tenemos 8 falsos positivos ==\n",
      "\n",
      "== Tenemos 0 falsos negativos ==\n",
      "\n",
      "== Tenemos un F1 Score de: 0.95 ==\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hacemos predicciones y evaluamos el modelo\n",
    "\n",
    "mis_predicciones = mi_modelo.predecir(X_test_scaled)\n",
    "cm = confusion_matrix(y_cancer_test, mis_predicciones)\n",
    "print(f'\\n=== Tenemos un accuracy de: {accuracy_score(y_cancer_test, mis_predicciones):.2f} ===\\n'\n",
    "      f'\\n== Tenemos {cm[0, 0]} verdaderos negativos ==\\n'\n",
    "      f'\\n== Tenemos {cm[1, 1]} verdaderos positivos ==\\n'\n",
    "      f'\\n== Tenemos {cm[0, 1]} falsos positivos ==\\n'   \n",
    "      f'\\n== Tenemos {cm[1, 0]} falsos negativos ==\\n'   \n",
    "      f'\\n== Tenemos un F1 Score de: {f1_score(y_cancer_test, mis_predicciones):.2f} ==\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298c679",
   "metadata": {},
   "source": [
    "¡Esto demuestra un buen desempeño del modelo! (Recordemos que es buena práctica realizar validación cruzada. ¡Lo haremos más adelante con el modelo propio de `scikit-lear`!)\n",
    "\n",
    " Ahora comparémoslo con el de `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01329695",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_sklearn = make_pipeline(MinMaxScaler(feature_range=(0, 1)), LogisticRegression())\n",
    "modelo_sklearn.fit(X_cancer_train, y_cancer_train)\n",
    "predicciones_sklearn = modelo_sklearn.predict(X_cancer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ae5dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== sklearn tiene un accuracy de: 0.98 ===\n",
      "\n",
      "== sklearn tiene 41 verdaderos negativos ==\n",
      "\n",
      "== sklearn tiene 71 verdaderos positivos ==\n",
      "\n",
      "== sklearn tiene 2 falsos positivos ==\n",
      "\n",
      "== sklearn tiene 0 falsos negativos ==\n",
      "\n",
      "== sklearn tiene un F1 Score de: 0.99 ==\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el modelo de sklearn\n",
    "\n",
    "cm_sklearn = confusion_matrix(y_cancer_test, predicciones_sklearn)\n",
    "print(f'\\n=== sklearn tiene un accuracy de: {accuracy_score(y_cancer_test, predicciones_sklearn):.2f} ===\\n'\n",
    "      f'\\n== sklearn tiene {cm_sklearn[0, 0]} verdaderos negativos ==\\n'\n",
    "      f'\\n== sklearn tiene {cm_sklearn[1, 1]} verdaderos positivos ==\\n'\n",
    "      f'\\n== sklearn tiene {cm_sklearn[0, 1]} falsos positivos ==\\n'   \n",
    "      f'\\n== sklearn tiene {cm_sklearn[1, 0]} falsos negativos ==\\n'   \n",
    "      f'\\n== sklearn tiene un F1 Score de: {f1_score(y_cancer_test, predicciones_sklearn):.2f} ==\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6abf64",
   "metadata": {},
   "source": [
    "El modelo es sospechosamente bueno. Investiguemos su capacidad de generalización para chequear que no haya overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d762ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores para desarrollo: [0.96610169 0.96610169 0.98245614 0.95798319 0.95798319]\n",
      "F1 promedio: 0.9661251833472015\n",
      "Desviación estándar de F1: 0.008936277353184214\n",
      "F1 FINAL en conjunto de prueba: 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "# Evaluamos el desempeño del modelo\n",
    "\n",
    "X_cancer_temp, X_cancer_test, y_cancer_temp, y_cancer_test = train_test_split(X_cancer, y_cancer, test_size=0.2, random_state=42)\n",
    "\n",
    "modelo_sklearn_cv = make_pipeline(MinMaxScaler(feature_range=(0, 1)), LogisticRegression())\n",
    "\n",
    "scores = cross_val_score(modelo_sklearn_cv, X_cancer_temp, y_cancer_temp, cv=5, scoring='f1')\n",
    "print(\"F1 scores para desarrollo:\", scores)\n",
    "print(\"F1 promedio:\", scores.mean())\n",
    "print(\"Desviación estándar de F1:\", scores.std())\n",
    "\n",
    "modelo_sklearn_cv.fit(X_cancer_temp, y_cancer_temp)\n",
    "final_f1 = f1_score(y_cancer_test, modelo_sklearn_cv.predict(X_cancer_test))\n",
    "print(\"F1 FINAL en conjunto de prueba:\", final_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba66c7",
   "metadata": {},
   "source": [
    "¡El modelo generaliza muy bien!\n",
    "\n",
    "**Buenas prácticas en el desarrollo de modelos de machine learning:**\n",
    "\n",
    "1. **Fase exploratoria (opcional):**  \n",
    "   Realiza pruebas rápidas con un solo conjunto de entrenamiento/prueba si necesitas depurar código o verificar que los modelos funcionan.  \n",
    "   No uses esta fase para tomar decisiones sobre qué modelo es mejor. Los resultados no son confiables ni representativos.\n",
    "\n",
    "2. **Fase de selección de modelos (validación cruzada):**  \n",
    "   Utiliza **validación cruzada** (por ejemplo, k-fold CV) en el conjunto de entrenamiento para:\n",
    "   - Comparar modelos y elegir el mejor,\n",
    "   - Ajustar hiperparámetros de forma objetiva,\n",
    "   - Medir el rendimiento esperable en datos nuevos,\n",
    "   sin haber tocado el conjunto de prueba.\n",
    "\n",
    "3. **Evaluación final:**  \n",
    "   Una vez elegido el modelo final, **entrena con todos los datos de entrenamiento** (sin CV)  \n",
    "   y **evalúalo una única vez** en el conjunto de prueba completamente separado.  \n",
    "   Esta evaluación representa el rendimiento real del modelo en producción.\n",
    "\n",
    "**Resumen:** No utilices el conjunto de prueba para guiar decisiones durante el desarrollo.  \n",
    "Solo úsalo al final para obtener una estimación honesta del rendimiento.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6317f1",
   "metadata": {},
   "source": [
    "## Parte 2: Árboles de Decisión\n",
    "\n",
    "Un **árbol de decisión** es un modelo de clasificación supervisada que predice la clase de una muestra a partir de una secuencia de preguntas binarias sobre sus características. La estructura del árbol está formada por **nodos de decisión**, donde se hace una pregunta del tipo “¿feature $x_j$ < umbral?”, y **hojas**, que contienen una predicción final (por ejemplo, clase 0 o clase 1).\n",
    "\n",
    "\n",
    "Durante el entrenamiento, el algoritmo construye el árbol de la siguiente forma: en cada nodo busca la característica y el umbral que mejor separen las clases. Es decir, quiere dividir los datos en subconjuntos que sean lo más puros posible (que contengan principalmente una sola clase). Para evaluar qué tan buena es una división, el algoritmo puede usar uno de dos **criterios de impureza**:\n",
    "\n",
    "La **entropía** mide el grado de incertidumbre de una distribución de clases en un conjunto de datos:\n",
    "\n",
    "$$\n",
    "H(D) = - \\sum_{k=1}^{K} p_k \\log_2(p_k)\n",
    "$$\n",
    "\n",
    "donde $p_k$ es la proporción de ejemplos de la clase $k$ en el conjunto $D$.\n",
    "\n",
    "La **ganancia de información** al hacer una división se define como:\n",
    "\n",
    "$$\n",
    "\\text{Gain}(D, \\text{split}) = H(D) - \\sum_{i=1}^{2} \\frac{|D_i|}{|D|} H(D_i)\n",
    "$$\n",
    "\n",
    "donde $D_1$ y $D_2$ son los subconjuntos después de la división. Este valor mide cuánto se reduce la entropía gracias a la división.\n",
    "\n",
    "\n",
    "Y la impureza según el **índice de Gini**, definido como:\n",
    "\n",
    "$$\n",
    "G(D) = 1 - \\sum_{k=1}^{K} p_k^2\n",
    "$$\n",
    "\n",
    "donde, nuevamente, $p_k$ es la proporción de ejemplos de clase $k$. Un valor cercano a 0 indica alta pureza (una sola clase dominante).\n",
    "\n",
    "\n",
    "El algoritmo de entrenamiento realiza los siguientes pasos:\n",
    "\n",
    "1. Comienza con todos los datos en la raíz del árbol.\n",
    "2. Busca la mejor división (característica y umbral) según el criterio escogido.\n",
    "3. Divide los datos y crea nodos hijos.\n",
    "4. Repite el proceso recursivamente para cada nodo hijo.\n",
    "5. Se detiene cuando:\n",
    "   - Los nodos contienen solo ejemplos de una clase,\n",
    "   - Se alcanza una profundidad máxima,\n",
    "   - O hay pocas muestras para continuar dividiendo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bfe8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos libreías necesarias\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc488600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "5  Adelie  Torgersen            39.3           20.6              190.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    Male  \n",
       "1       3800.0  Female  \n",
       "2       3250.0  Female  \n",
       "4       3450.0  Female  \n",
       "5       3650.0    Male  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el dataset de pingüinos de seaborn y lo convertimos a DataFrame de pandas\n",
    "\n",
    "df_penguins = sns.load_dataset('penguins').dropna()\n",
    "df_penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489b83b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_penguins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed328043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male' 'Female']\n",
      "sex\n",
      "Male      168\n",
      "Female    165\n",
      "Name: count, dtype: int64\n",
      "<bound method Series.unique of species               object\n",
      "island                object\n",
      "bill_length_mm       float64\n",
      "bill_depth_mm        float64\n",
      "flipper_length_mm    float64\n",
      "body_mass_g          float64\n",
      "sex                   object\n",
      "dtype: object>\n"
     ]
    }
   ],
   "source": [
    "# Analizamos la variable objetivo\n",
    "\n",
    "print(df_penguins['sex'].unique())\n",
    "print(df_penguins['sex'].value_counts())\n",
    "print(df_penguins.dtypes.unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cc1e7",
   "metadata": {},
   "source": [
    "¡Los datos están balanceados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8556d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['species', 'island'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codificamos las características categóricas\n",
    "\n",
    "categorical_cols = df_penguins.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_cols.drop('sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5510bc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "5  Adelie  Torgersen            39.3           20.6              190.0   \n",
       "\n",
       "   body_mass_g  sex  \n",
       "0       3750.0    1  \n",
       "1       3800.0    0  \n",
       "2       3250.0    0  \n",
       "4       3450.0    0  \n",
       "5       3650.0    1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codificamos la variable objetivo\n",
    "df_penguins['sex'] = df_penguins['sex'].map({'Female': 0, 'Male': 1})\n",
    "df_penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c05c5682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>species_Chinstrap</th>\n",
       "      <th>species_Gentoo</th>\n",
       "      <th>island_Dream</th>\n",
       "      <th>island_Torgersen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0            39.1           18.7              181.0       3750.0   \n",
       "1            39.5           17.4              186.0       3800.0   \n",
       "2            40.3           18.0              195.0       3250.0   \n",
       "4            36.7           19.3              193.0       3450.0   \n",
       "5            39.3           20.6              190.0       3650.0   \n",
       "\n",
       "   species_Chinstrap  species_Gentoo  island_Dream  island_Torgersen  \n",
       "0              False           False         False              True  \n",
       "1              False           False         False              True  \n",
       "2              False           False         False              True  \n",
       "4              False           False         False              True  \n",
       "5              False           False         False              True  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparamos los datos para el modelo\n",
    "\n",
    "y_penguins = df_penguins['sex'].to_numpy()\n",
    "X_t = df_penguins.drop(columns=['sex'])\n",
    "X_t = pd.get_dummies(X_t, drop_first=True) # Codificamos las características categóricas usando one-hot encoding\n",
    "X_penguins = X_t.to_numpy()\n",
    "X_penguins_temp, X_penguins_test, y_penguins_temp, y_penguins_test = train_test_split(X_penguins, y_penguins, test_size=0.2, random_state=42)\n",
    "X_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adf20767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores en validación cruzada: [0.88888889 0.89655172 0.84210526 0.88135593 0.89285714]\n",
      "F1 promedio (desarrollo): 0.8803517902490494\n",
      "Desviación estándar: 0.019776293336427497\n",
      "F1 FINAL en conjunto de prueba: 0.84375\n"
     ]
    }
   ],
   "source": [
    "# Creamos y evaluamos el modelo\n",
    "\n",
    "modelo_arbol = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=3,              # profundidad máxima del árbol\n",
    "    min_samples_split=10,     # mínimo de muestras para dividir un nodo\n",
    "    min_samples_leaf=5,       # mínimo de muestras por hoja\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Validación cruzada sobre el conjunto de desarrollo\n",
    "scores = cross_val_score(modelo_arbol, X_penguins_temp, y_penguins_temp, cv=5, scoring='f1')\n",
    "print(\"F1 scores en validación cruzada:\", scores)\n",
    "print(\"F1 promedio (desarrollo):\", scores.mean())\n",
    "print(\"Desviación estándar:\", scores.std())\n",
    "\n",
    "# Entrenar el modelo final con todos los datos de desarrollo\n",
    "modelo_arbol.fit(X_penguins_temp, y_penguins_temp)\n",
    "\n",
    "# Paso 5: Evaluar una única vez en el conjunto de prueba\n",
    "y_pred = modelo_arbol.predict(X_penguins_test)\n",
    "final_f1 = f1_score(y_penguins_test, y_pred)\n",
    "print(\"F1 FINAL en conjunto de prueba:\", final_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db2a69",
   "metadata": {},
   "source": [
    "Afinemos los hiperpámetros del modelo para ver si encontramos unos que mejoren el desempeño de manera significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1e0e331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados:\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "F1 FINAL del mejor modelo en el conjunto de prueba: 0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "# Definimos el espacio de búsqueda\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Gini vs Información\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Usamos el mismo clasificador base\n",
    "modelo_base = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Definimos la búsqueda\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ejecutamos la búsqueda sobre el conjunto de desarrollo\n",
    "grid_search.fit(X_penguins_temp, y_penguins_temp)\n",
    "\n",
    "# Mostramos los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Evaluamos el mejor modelo en el conjunto de prueba\n",
    "mejor_modelo = grid_search.best_estimator_\n",
    "f1_final = f1_score(y_penguins_test, mejor_modelo.predict(X_penguins_test))\n",
    "print(\"F1 FINAL del mejor modelo en el conjunto de prueba:\", f1_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fae8b5",
   "metadata": {},
   "source": [
    "**Comparación de modelos y capacidad de generalización**\n",
    "\n",
    "Al comparar un modelo ajustado manualmente con hiperparámetros conservadores y otro optimizado mediante `GridSearchCV`, es importante no confundir \"mejor validación cruzada\" con \"mejor capacidad de generalización\".\n",
    "\n",
    "\n",
    "\n",
    "**Definición clave**\n",
    "\n",
    "> **Capacidad de generalización**: Es la habilidad de un modelo para mantener un rendimiento consistente al enfrentarse a datos nuevos y no vistos durante el entrenamiento o la validación.\n",
    "\n",
    "\n",
    "**Resultados comparados**\n",
    "\n",
    "| Métrica                      | Modelo conservador | Modelo GridSearch |\n",
    "|-----------------------------|--------------------|-------------------|\n",
    "| **F1 promedio (CV)**         | 0.880              | ~0.82 (estimado)  |\n",
    "| **F1 final (test)**          | 0.844              | 0.820             |\n",
    "| **Brecha de generalización** | 0.036              | ≈0.00             |\n",
    "\n",
    "\n",
    "**Interpretación**\n",
    "\n",
    "- Aunque el modelo de GridSearch parece tener una \"brecha\" menor entre CV y test, **esto no implica que generalice mejor**.\n",
    "- De hecho, el modelo de GridSearch:\n",
    "  - Tuvo **peor rendimiento en validación** y en test,\n",
    "  - Probablemente **sobreajustó** los folds de validación cruzada,\n",
    "  - Escogió hiperparámetros más flexibles (ej. `min_samples_leaf=1`), lo que aumentó el riesgo de sobreajuste.\n",
    "\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "> El modelo manual, con hiperparámetros conservadores, logró **mejor rendimiento final** y mostró una **mayor capacidad de generalización**.  \n",
    ">  \n",
    "> Este es un ejemplo clásico de cómo una estrategia de regularización simple y prudente puede superar a una optimización agresiva, especialmente cuando el volumen de datos es limitado.\n",
    "\n",
    "\n",
    "\n",
    "**Consejo final:** No solo se debe mirar la métrica final — analiza también cómo varía entre entrenamiento, validación y prueba. ¡La estabilidad es clave para modelos confiables!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623aaa4",
   "metadata": {},
   "source": [
    "¿Tenemos otras opciones para mejorar el desempeño de nuestro modelo basado en árboles de decisión?\n",
    "\n",
    "**¡Sí! Podemos usar un modelo de _Random Forest_**\n",
    "\n",
    "El modelo de **Random Forest** es una técnica de **ensamble**, es decir, combina **muchos árboles de decisión** en lugar de usar solo uno. En lugar de confiar en un único árbol (que puede sobreajustarse o ser inestable), el bosque genera múltiples árboles y **promedia sus predicciones**. Funciona asíÑ\n",
    "\n",
    "1. **Bootstrap:** Crea muchas muestras aleatorias del conjunto de entrenamiento original (con reemplazo).\n",
    "2. **Entrena muchos árboles:** Cada árbol se entrena sobre una muestra diferente, usando solo un subconjunto aleatorio de características en cada división.\n",
    "3. **Combina predicciones:** En clasificación, cada árbol vota y se toma la clase más votada; en regresión, se hace un promedio.\n",
    "\n",
    "Las ventajas incluyen:\n",
    "\n",
    "- **Reduce el sobreajuste:** Al promediar muchos árboles diferentes, se suavizan los errores de modelos individuales.\n",
    "- **Mejor generalización:** La variación entre los árboles mejora la robustez frente a nuevos datos.\n",
    "\n",
    "En resumen:\n",
    "\n",
    "> **Random Forest = muchos árboles débiles → un modelo fuerte, más preciso y más estable.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da8605cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_rf, X_test_rf, y_temp_rf, y_test_rf = train_test_split(X_penguins, y_penguins, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01aacf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores en validación cruzada: [0.94736842 0.81632653 0.89655172 0.89285714 0.94545455]\n",
      "F1 promedio (desarrollo): 0.8997116728228992\n",
      "Desviación estándar: 0.04769408316900793\n",
      "F1 FINAL en conjunto de prueba: 0.8709677419354839\n"
     ]
    }
   ],
   "source": [
    "# Modelo de random forest\n",
    "\n",
    "modelo_bosque = RandomForestClassifier(\n",
    "    n_estimators=100,         # número de árboles en el bosque\n",
    "    max_depth=5,              # profundidad máxima de cada árbol\n",
    "    min_samples_split=10,     # mínimo de muestras para dividir un nodo\n",
    "    min_samples_leaf=5,       # mínimo de muestras en una hoja\n",
    "    random_state=42,\n",
    "    n_jobs=-1                 # usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Paso 3: Validación cruzada sobre el conjunto de desarrollo\n",
    "scores_rf = cross_val_score(modelo_bosque, X_temp_rf, y_temp_rf, cv=5, scoring='f1')\n",
    "print(\"F1 scores en validación cruzada:\", scores_rf)\n",
    "print(\"F1 promedio (desarrollo):\", scores_rf.mean())\n",
    "print(\"Desviación estándar:\", scores_rf.std())\n",
    "\n",
    "modelo_bosque.fit(X_temp_rf, y_temp_rf)\n",
    "\n",
    "y_pred_rf = modelo_bosque.predict(X_test_rf)\n",
    "final_f1_rf = f1_score(y_test_rf, y_pred_rf)\n",
    "print(\"F1 FINAL en conjunto de prueba:\", final_f1_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5efeff",
   "metadata": {},
   "source": [
    "Tratemos de mejorar el modelo con `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f45d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'criterion': 'entropy', 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "F1 FINAL del mejor modelo en el conjunto de prueba: 0.8709677419354839\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "modelo_base = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_temp_rf, y_temp_rf)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "mejor_modelo = grid_search.best_estimator_\n",
    "final_f1 = f1_score(y_test_rf, mejor_modelo.predict(X_test_rf))\n",
    "print(\"F1 FINAL del mejor modelo en el conjunto de prueba:\", final_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curso_KPMG_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
